export const en = {
  nav: {
    dashboard: 'Dashboard',
    assessment: 'Assessment',
    results: 'Results',
  },
  dashboard: {
    title: 'Maturity Model for High-Risk AI Governance',
    subtitle: 'Self-assessment of organizational AI governance maturity according to Art. 9\u201315 EU AI Act (Regulation 2024/1689) for high-risk AI systems.',
    criteriaCount: 'Criteria',
    clickForDetails: 'Click for details \u2192',
    howItWorksTitle: 'How the Assessment Works',
    step1Label: 'Rate 31 criteria',
    step1Desc: '\u2014 six dimensions (D1\u2013D6)',
    step2Label: 'Automatic scoring',
    step2Desc: '\u2014 dimension & overall scores',
    step3Label: 'Gap analysis',
    step3Desc: '\u2014 prioritized action recommendations',
    startAssessment: 'Start Assessment',
    valueMaturity: 'Maturity Score (CMMI)',
    valueGapAnalysis: 'Gap Analysis with Priorities',
    valueActionPlan: 'Action Plan with Quick Wins',
    valueRegulatory: 'Regulatory Risk Assessment',
    timeEstimate: 'approx. 15\u201320 min (individual)',
    progressTitle: 'Assessment in Progress',
    progressDesc: '{rated} of {total} criteria rated',
    progressLabel: 'Assessment progress',
    continueAssessment: 'Continue',
    resultsAvailable: 'Results Available',
    resultsAvailableDesc: 'Your last assessment result is saved.',
    viewResults: 'View Results',
    resetAssessment: 'Reset',
    historyTitle: 'Previous Assessments',
    loadError: 'Could not load dimensions. Is the backend running?',
    retry: 'Try again',
    valuePropositions: 'Assessment features',
    methodologyTitle: 'Scientific Basis',
    methodologyDesc: 'This assessment is based on a maturity model developed as part of a Master\'s thesis in the M.Sc. Technology & Management program at Provadis School of International Management & Technology in collaboration with Accenture. It combines requirements from the EU AI Act (Regulation 2024/1689, Art. 9–15) with the CMMI maturity approach (5 levels), evaluating six governance dimensions across 31 criteria.',
    methodologyScoring: 'Scoring: DimScore = arithmetic mean of rated criteria per dimension · OverallScore = weighted sum of all DimScores · Gap analysis with risk-category-dependent thresholds',
    methodologySources: 'Reference framework: EU AI Act (Reg. 2024/1689) · CMMI Institute (2018) · ISO/IEC 42001:2023 · NIST AI RMF 1.0',
  },
  assessment: {
    summary: 'Summary',
    scoping: 'Scoping',
    overview: 'Overview',
    stepOf: 'Step {current} of {total}',
    scopingTitle: 'Step 1: Scoping',
    scopingDesc: 'Define the AI system to be assessed and the organizational context.',
    extendedScopingDesc: 'Extended Details \u2014 Optional information for more precise results.',
    systemNameLabel: 'AI System Name *',
    systemNamePlaceholder: 'e.g. Credit Scoring System',
    riskCategoryLabel: 'Risk Category',
    riskCategoryHighRisk: 'High-Risk (Art. 6, Annex III)',
    riskCategoryLimited: 'Limited Risk',
    riskCategoryMinimal: 'Minimal Risk',
    industryLabel: 'Industry',
    industryPlaceholder: 'e.g. Financial Services',
    orgSizeLabel: 'Organization Size',
    orgSizeSmall: 'Small (<50 employees)',
    orgSizeMedium: 'Medium (50\u2013250 employees)',
    orgSizeLarge: 'Large (>250 employees)',
    deploymentLabel: 'Deployment Status',
    deploymentProduction: 'Production',
    deploymentPreDeployment: 'Pre-Deployment',
    startAssessment: 'Start Assessment \u2192',
    riskExplanation: {
      'high-risk': '\ud83d\udfe5 High-Risk AI (Annex III): e.g. credit scoring, recruitment, biometric identification, medical diagnosis. Requires conformity assessment and full compliance with Art. 9\u201315.',
      'limited-risk': '\ud83d\udfe8 Limited Risk (Art. 50): e.g. chatbots, emotion recognition, deepfake generation. Transparency obligations, but no full conformity assessment required.',
      'minimal-risk': '\ud83d\udfe9 Minimal Risk: e.g. spam filters, recommendation systems, search engines. Voluntary codes of conduct, no specific obligations under the EU AI Act.',
    },
    mandatoryBadge: 'Mandatory for {risk}',
    naReasonPlaceholder: 'Justification for N/A (optional)...',
    naReasonHint: 'Select N/A only if this criterion is not applicable to your system.',
    levelExplanationTitle: 'What do the levels mean?',
    regulatoryContext: 'Regulatory Context',
    weightingTitle: 'Adjust dimension weighting (optional)',
    weightingDesc: 'Adjust the relative importance of dimensions to your context. Default: 1.0 (equal weighting). High-risk systems should weight D1 and D6 higher.',
    back: '\u2190 Back',
    next: 'Next: {name} \u2192',
    toSummary: 'To Summary \u2192',
    summaryTitle: 'Assessment Summary',
    summaryDesc: 'Review your ratings before completing the assessment.',
    systemLabel: 'System:',
    industryLabelSummary: 'Industry:',
    riskCategoryLabelSummary: 'Risk Category:',
    sizeLabel: 'Size:',
    clickToEdit: 'click to edit',
    backToD6: '\u2190 Back to D6',
    submitting: 'Evaluating...',
    submitAssessment: 'Complete Assessment \u2713',
    errorSubmit: 'Error during evaluation. Is the backend server running?',
  },
  results: {
    noResultsTitle: 'No Results Available',
    noResultsDesc: 'Complete an assessment first to view results.',
    startAssessment: 'Start Assessment',
  },
  bento: {
    title: 'Assessment Results',
    overallMaturity: 'Overall Maturity Level',
    outOf: 'out of 5.0 maximum',
    governanceProfile: 'Governance Profile',
    gapAnalysis: 'Gap Analysis',
    priorityActions: 'Priority Action Areas',
    dimensionScores: 'Dimension Scores',
    dimensionHeader: 'Dimension',
    scoreHeader: 'Score',
    levelHeader: 'Level',
    statusHeader: 'Status',
    maturityDetail: 'Maturity Detail',
    levelLabel: 'Level {n}: {name}',
    cmmiNote: 'CMMI-based (5 levels)',
    actions: 'Actions',
    newAssessment: 'New Assessment',
    printResults: 'Print Results',
    exportJSON: 'Export JSON',
    shareLink: 'Create Shareable Link',
    linkCopied: 'Link copied!',
    pdfDownload: 'Download PDF Report',
    pdfGenerating: 'Generating PDF\u2026',
    dimensionsRated: '{n} dimensions rated',
    // B2: Card filter
    filterCards: 'Show/hide cards',
    hidden: 'hidden',
    selectCards: 'Select visible cards',
    resetFilter: 'Show all',
    cardExecutiveSummary: 'Executive Summary',
    cardOverallScore: 'Overall Score',
    cardRadarChart: 'Radar Chart',
    cardArticleCompliance: 'Article Compliance',
    cardGapAnalysis: 'Gap Analysis',
    cardDimensionScores: 'Dimension Scores',
    cardComplianceHeatmap: 'Compliance Heatmap',
    cardRoadmapTimeline: 'Roadmap',
    cardArticleMapping: 'Article Mapping',
    cardComplianceTimeline: 'Compliance Timeline',
    cardGlossary: 'Glossary',
    cardRACIMatrix: 'RACI Matrix',
    cardMaturityDetail: 'Maturity Detail',
    cardMaturityDist: 'Maturity Distribution',
    cardActions: 'Actions',
  },
  gap: {
    noGaps: 'No critical governance gaps identified (all dimensions \u2265 3.0).',
    showActionPlan: 'Show Action Plan',
    severityCritical: 'Critical',
    severitySignificant: 'Significant',
    severityModerate: 'Moderate',
    regulatoryRisk: 'Regulatory Risk',
    dependencyWarning: 'Dependency',
    quickWinsLabel: 'Quick Wins \u2014 immediately actionable',
    dep_D5_D1: 'D5 (Human Oversight) builds on D1 (Risk Management). Without risk context, no meaningful oversight design is possible. Recommendation: bring D1 to Level 3 first.',
    dep_D6_D1: 'D6 (Technical Robustness) requires D1 (Risk Management). Robustness requirements are derived from risk assessment. Recommendation: address D1 first.',
    dep_D4_D3: 'D4 (Transparency) relies on D3 (Documentation). Without complete technical documentation, adequate transparency cannot be ensured.',
  },
  regulatory: {
    D1: 'Potential violation of Art. 9 EU AI Act (Regulation 2024/1689). For high-risk AI systems, a documented risk management system maintained throughout the entire lifecycle is mandatory. Sanctions: up to 3% of global annual turnover (Art. 99(3)).',
    D2: 'Potential violation of Art. 10 EU AI Act. Training, validation and test data must meet defined quality criteria, including governance, representativeness and bias testing. Sanctions: up to 3% of annual turnover (Art. 99(3)).',
    D3: 'Potential violation of Art. 11\u201312 EU AI Act. Technical documentation and automatic logging are mandatory before placing on the market and must be maintained throughout the lifecycle. Sanctions: up to 3% of annual turnover.',
    D4: 'Potential violation of Art. 13 EU AI Act. High-risk AI systems must be designed so that their operation is sufficiently transparent and comprehensible for users. Lack of transparency may result in fines of up to 3% of annual turnover.',
    D5: 'Potential violation of Art. 14 EU AI Act. Effective human oversight must be ensured, including qualified oversight personnel and intervention capabilities. Sanctions: up to 3% of global annual turnover.',
    D6: 'Potential violation of Art. 15 EU AI Act. High-risk AI systems must achieve an appropriate level of accuracy, robustness and cybersecurity. Sanctions: up to 3% of global annual turnover (Art. 99(3)).',
  },
  modal: {
    close: 'Close',
    assessmentCriteria: 'Assessment Criteria',
    assessThisDimension: 'Assess this dimension \u2192',
    articleDescriptions: {
      'Art. 9': 'Risk Management \u2014 Art. 9 AI Act: High-risk AI systems require a continuous risk management system throughout the entire lifecycle.',
      'Art. 10': 'Data Quality \u2014 Art. 10 AI Act: Training, validation, and test datasets must meet relevant quality criteria.',
      'Art. 11': 'Technical Documentation \u2014 Art. 11 AI Act: Comprehensive technical documentation must be drawn up before placing on the market.',
      'Art. 12': 'Record-Keeping \u2014 Art. 12 AI Act: Automatic logging must be ensured throughout the entire lifecycle.',
      'Art. 13': 'Transparency \u2014 Art. 13 AI Act: High-risk AI systems must be designed to be sufficiently transparent in their operation.',
      'Art. 14': 'Human Oversight \u2014 Art. 14 AI Act: High-risk AI systems must enable effective human oversight.',
      'Art. 15': 'Accuracy & Robustness \u2014 Art. 15 AI Act: High-risk AI systems must achieve an appropriate level of accuracy, robustness, and cybersecurity.',
      'Art. 11\u201312': 'Documentation & Logging \u2014 Art. 11\u201312 AI Act: Technical documentation and automatic record-keeping obligations for high-risk AI systems.',
      'Art. 14\u201315': 'Oversight & Robustness \u2014 Art. 14\u201315 AI Act: Human oversight, accuracy, robustness, and cybersecurity for high-risk AI systems.',
    },
  },
  chat: {
    title: 'AI Governance Assistant',
    ragLabel: 'RAG-powered',
    allDimensions: 'All Dimensions',
    placeholder: 'Ask a question...',
    openAssistant: 'Open AI Governance Assistant',
    emptyState: 'Ask questions about the assessment, dimensions, or the EU AI Act.',
    errorMessage: 'Error connecting to the AI assistant. Is the API key configured in the .env file?',
    starterPrompts: [
      'What does this dimension mean in practice?',
      'What are the best practices?',
      'How do I achieve a higher maturity level?',
    ],
    resultsPrompts: [
      'Analyze my results and give me an assessment.',
      'Where should I start first?',
      'What quick wins can I implement immediately?',
    ],
    dashboardPrompts: [
      'What is the EU AI Act and what does it mean for me?',
      'How does an assessment work?',
      'What are the 6 governance dimensions?',
    ],
    dimensionPrompts: [
      'Explain dimension {dim} in detail.',
      'What are the best practices for {dim}?',
      'How do I reach Level 3 in {dim}?',
    ],
    analyzePrompt: 'Please analyze my assessment results. Give me an overall assessment, identify the key strengths and weaknesses, and recommend concrete next steps.',
  },
  scoreSlider: {
    level: 'Level {n}',
    notRated: 'Not rated',
  },
  radar: {
    tooltipScore: 'Score',
    radarName: 'Governance Maturity',
    targetLabel: 'Target (Minimum Conformity)',
    bestPractice: 'Best Practice',
  },
  actionPlan: {
    title: 'Action Plan',
    implementationSteps: 'Implementation Steps',
    quickWinsTitle: 'Quick Wins \u2014 immediately actionable',
    sourcesTitle: 'Further Resources',
    d1: {
      name: 'Risk Management',
      article: 'Art. 9',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Create an AI risk register', description: 'Create a central spreadsheet (e.g. Excel/Notion) with columns: Risk ID, Description, Likelihood (1\u20135), Impact (1\u20135), Risk Score, Owner, Status. List at least 10 AI-specific risks (e.g. bias, drift, data leaks, adversarial attacks).' },
            { title: 'Assign risk owners', description: 'Assign a responsible person to each identified risk. This person does not need to implement all measures themselves but must maintain oversight and report regularly.' },
            { title: 'Conduct initial risk assessment', description: 'Conduct a workshop with relevant stakeholders. Use a simple risk matrix (likelihood x severity) and prioritize the top 5 risks for immediate treatment.' },
            { title: 'Define basic mitigation measures', description: 'Define at least one countermeasure for each of the top 5 risks. Document: What will be done? Who is responsible? By when? How will effectiveness be measured?' },
          ],
          quickWins: [
            'Download and fill in an AI risk template',
            'Introduce a monthly 30-minute risk review in the team',
            'Use ISO 31000 risk categories as a template',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Document a formal AI RMS', description: 'Create a written AI risk management handbook covering: scope, roles & responsibilities, risk assessment methodology (e.g. FMEA for AI), escalation processes, review cycles. Align with ISO/IEC 23894 (AI Risk Management).' },
            { title: 'Ensure lifecycle coverage', description: 'Ensure risks are identified at every phase: design, data collection, training, validation, deployment, operation, decommissioning. Use checklists for each phase.' },
            { title: 'Create a risk treatment plan', description: 'Create a systematic plan with measures (avoid, mitigate, transfer, accept) and documented residual risk acceptance criteria. Define explicit thresholds above which a risk is not acceptable.' },
            { title: 'Implement testing procedures', description: 'Define regular testing procedures: robustness tests, fairness tests, stress tests. Document test plans, results, and derived measures.' },
          ],
          quickWins: [
            'Adapt ISO/IEC 23894 as a framework',
            'Introduce quarterly risk review meetings',
            'Create a risk treatment matrix with RACI assignment',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Define risk management KPIs', description: 'Define measurable indicators: number of identified risks, average time-to-mitigation, proportion of treated vs. open risks, deviation rate in risk assessments. Visualize these in a dashboard.' },
            { title: 'Introduce quantitative risk analysis', description: 'Supplement qualitative assessment with statistical methods: Monte Carlo simulations for impact analyses, Bayesian networks for risk dependencies, sensitivity analyses.' },
            { title: 'Build automated monitoring', description: 'Implement automated threshold monitoring for the most important risk indicators. Set up alerting mechanisms for threshold breaches.' },
            { title: 'Establish effectiveness measurement', description: 'Systematically measure whether implemented measures actually reduce risks. Use before-and-after comparisons and statistical tests.' },
          ],
          quickWins: [
            'Set up a risk dashboard with top 10 KPIs',
            'Automatic alerts on threshold breaches',
            'Send weekly risk metrics by email',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Predictive risk identification', description: 'Use ML-based methods to predict emerging risks based on historical data, trend analyses, and external signals (e.g. regulatory changes, new attack methods).' },
            { title: 'Continuous optimization cycles', description: 'Implement PDCA cycles (Plan-Do-Check-Act) with data-driven decision-making. Each cycle should demonstrate measurable improvements in the risk profile.' },
            { title: 'Best practice sharing', description: 'Establish a process for sharing lessons learned within the organization and with the community (e.g. through anonymized case studies, conference contributions, industry working groups).' },
            { title: 'Real-time risk control', description: 'Implement real-time monitoring with automated adjustment of risk parameters. The system should be able to independently escalate measures when thresholds are exceeded.' },
          ],
          quickWins: [
            'Trend analysis of the last 12 months of risk data',
            'Lessons-learned workshop after each incident',
            'Benchmarking with industry peer group',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'Regulatory horizon scanning', description: 'Actively track regulatory developments (EU AI Act implementing acts, new harmonized standards, national supervisory authority guidelines) and anticipate their impact on your risk management system.' },
            { title: 'Cross-industry benchmarking', description: 'Regularly compare your RMS with best-in-class approaches from other industries. Adapt proven practices from related fields (e.g. medical devices, aviation, nuclear safety).' },
            { title: 'Proactively assess innovation risks', description: 'Systematically evaluate new AI technologies (foundation models, multimodal systems) for new risk categories before adoption and proactively adjust your RMS.' },
          ],
          quickWins: [
            'Subscribe to a regulatory newsletter (e.g. AI Act Observer)',
            'Conduct an annual external audit of the RMS',
            'Establish a research collaboration with a university',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 9 Risk Management System (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'ISO/IEC 23894:2023 \u2014 AI Risk Management', url: 'https://www.iso.org/standard/77304.html' },
        { title: 'NIST AI Risk Management Framework', url: 'https://www.nist.gov/artificial-intelligence/ai-risk-management-framework' },
        { title: 'BSI \u2014 AI Risk Assessment Guide', url: 'https://www.bsi.bund.de/DE/Themen/Unternehmen-und-Organisationen/Informationen-und-Empfehlungen/Kuenstliche-Intelligenz/kuenstliche-intelligenz_node.html' },
      ],
    },
    d2: {
      name: 'Data Governance',
      article: 'Art. 10',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Create a data inventory', description: 'Create an overview of all datasets used: source, scope, collection period, personal data references, purpose of use. Use a simple spreadsheet as a data catalog.' },
            { title: 'Introduce basic quality checks', description: 'Implement automated checks for: missing values, duplicates, outliers, data type consistency. Tools like Great Expectations or pandas-profiling facilitate getting started.' },
            { title: 'Ensure GDPR basics', description: 'Verify for each dataset: legal basis, purpose limitation, retention periods, right of access capability. Document this in a record of processing activities (Art. 30 GDPR).' },
          ],
          quickWins: [
            'Generate a data profiling report for each dataset',
            'Create a record of processing activities per Art. 30 GDPR',
            'Automate basic data validation rules',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Document data quality standards', description: 'Define measurable quality criteria: completeness (>95%), timeliness (<30 days), consistency (0% contradictions), accuracy (sample validation). Document thresholds and escalation processes.' },
            { title: 'Systematize bias analysis', description: 'Implement fairness metrics (Demographic Parity, Equalized Odds, etc.) as part of the data pipeline. Conduct bias audits for all training datasets.' },
            { title: 'Introduce data lineage', description: 'Document the complete lifecycle of each dataset: origin \u2192 preparation \u2192 transformation \u2192 usage \u2192 archival. Tools like Apache Atlas or OpenLineage support automation.' },
            { title: 'Implement privacy-by-design', description: 'Integrate data protection into the data pipeline: pseudonymization, data anonymization, access controls, audit logging. Conduct DPIAs (Data Protection Impact Assessments) for AI systems.' },
          ],
          quickWins: [
            'Set up a fairness dashboard for training data',
            'Automated data quality checks in CI/CD pipeline',
            'Create a DPIA template for AI systems',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Automated quality monitoring', description: 'Implement dashboards displaying data quality metrics in real time: drift detection on input data, quality trends, anomaly detection. Set automatic alerts for quality violations.' },
            { title: 'Automated bias detection', description: 'Integrate fairness checks as automated tests in your ML pipeline. Every model update should be automatically checked for bias before deployment.' },
            { title: 'Automated lineage capture', description: 'Capture data lineage automatically by instrumenting the data pipeline. Every transformation is automatically documented and visualized.' },
          ],
          quickWins: [
            'Introduce Great Expectations as data validation framework',
            'Integrate automated fairness testing in CI/CD',
            'Agree on data quality SLAs with data suppliers',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Predictive data quality', description: 'Use ML models to predict data quality problems before they occur. Train models on historical quality metrics to forecast degradation.' },
            { title: 'Automated bias correction', description: 'Implement automated resampling, reweighting, or adversarial debiasing procedures that automatically engage when bias is detected.' },
            { title: 'Impact analysis for data changes', description: 'Automate the analysis of which models and decisions are affected by changes to datasets. Use complete lineage graphs for impact propagation.' },
          ],
          quickWins: [
            'ML-based anomaly detection on data streams',
            'Automated re-training triggers on data drift',
            'Data-quality-as-code in version control',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'Evaluate synthetic data', description: 'Assess the use of synthetic data to supplement real data, especially for underrepresented groups. Systematically validate the quality of synthetic data.' },
            { title: 'Evaluate federated learning', description: 'Assess privacy-friendly training methods such as federated learning or differential privacy for sensitive datasets.' },
          ],
          quickWins: [
            'Automate data governance report for stakeholders',
            'Participate in data quality challenge benchmarks',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 10 Data and Data Governance (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'DAMA DMBOK \u2014 Data Management Body of Knowledge', url: 'https://www.dama.org/cpages/body-of-knowledge' },
        { title: 'Google \u2014 Responsible AI Practices: Fairness', url: 'https://ai.google/responsibility/responsible-ai-practices/' },
        { title: 'EDPB \u2014 Guidelines on AI and Data Protection', url: 'https://edpb.europa.eu/' },
      ],
    },
    d3: {
      name: 'Documentation',
      article: 'Art. 11\u201312',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Create a documentation template', description: 'Create a template based on Annex IV of the AI Act: general description, system characteristics, algorithm description, data description, validation results, monitoring measures. Fill it initially with existing knowledge.' },
            { title: 'Set up basic logging', description: 'Implement structured logging for: model inference requests, system events, error cases, usage statistics. Use a common format (e.g. JSON) and centralize logs.' },
            { title: 'Start versioning', description: 'Introduce basic versioning: Git for code, DVC or MLflow for models and datasets. Document changes in a changelog with each release.' },
          ],
          quickWins: [
            'Use Annex IV of the AI Act as a documentation template',
            'Implement structured JSON logging',
            'Introduce Git-based versioning for all artifacts',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Complete Annex IV documentation', description: 'Ensure all points from Annex IV are covered: system description, design decisions, training and test methodology, performance metrics, limitations, monitoring plans.' },
            { title: 'Systematic event logging', description: 'Implement comprehensive logging: all AI system decisions, input data (references), confidence values, explanations, manual overrides, system states. Ensure retention periods.' },
            { title: 'End-to-end versioning of all artifacts', description: 'Systematically version: training data, model parameters, configurations, feature engineering pipelines, evaluation results. Every model version must be reproducible.' },
            { title: 'Systematically create conformity evidence', description: 'Create templates for regulatory reports and declarations of conformity. Define the creation process, responsible persons, and review cycles.' },
          ],
          quickWins: [
            'Introduce MLflow/Weights & Biases for experiment tracking',
            'Define and implement a log retention policy',
            'Create a model card template for each model',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Auto-generated documentation', description: 'Automate documentation creation: model cards generated from experiment tracking, API documentation from code, data sheets from data profiling.' },
            { title: 'Centralized log management', description: 'Implement a central log platform (e.g. ELK Stack, Grafana Loki) with: search, filtering, analysis dashboards, automated anomaly detection in logs.' },
            { title: 'Automated conformity evidence', description: 'Generate conformity reports automatically from the system: test reports, performance reports, audit trails. Ensure seamless traceability.' },
          ],
          quickWins: [
            'CI/CD pipeline automatically generates model cards',
            'ELK Stack or Grafana Loki as log platform',
            'Set up automated compliance report generation',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Living documentation', description: 'Implement documentation that automatically updates when the system changes. Automatically validate completeness and timeliness.' },
            { title: 'Intelligent logging', description: 'Use ML for anomaly detection in logs, automatic event correlation, and predictive warning of system failures.' },
            { title: 'Full reproducibility', description: 'Ensure that every historical model version can be reproduced with the exact same data, code, and configurations. Automate reproducibility tests.' },
          ],
          quickWins: [
            'Fully implement documentation-as-code approach',
            'Automated documentation completeness checks',
            'Reproducibility test as part of the CI/CD pipeline',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'AI Bill of Materials (AI-BOM)', description: 'Create a machine-readable AI-BOM that captures all components, dependencies, and licenses of your AI system \u2014 analogous to the Software BOM (SBOM).' },
            { title: 'Prepare for interoperability', description: 'Prepare your documentation for future EU standardization. Use open formats and interfaces.' },
          ],
          quickWins: [
            'Create an AI-BOM prototype',
            'Track CEN/CENELEC AI standards',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 11\u201312, Annex IV (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'Google Model Cards for Model Reporting', url: 'https://modelcards.withgoogle.com/about' },
        { title: 'MLflow \u2014 Open-Source ML Lifecycle Platform', url: 'https://mlflow.org/' },
        { title: 'OECD \u2014 Framework for AI System Classification', url: 'https://oecd.ai/en/classification' },
      ],
    },
    d4: {
      name: 'Transparency',
      article: 'Art. 13',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Create a system description for users', description: 'Write a comprehensible description: What does the system do? How does it make decisions? What data does it use? What are its limitations? Use clear language, no technical jargon.' },
            { title: 'Introduce AI labeling', description: 'Label all touchpoints where AI is used. Users must know when they are interacting with an AI system (Art. 50 AI Act for certain systems).' },
            { title: 'Provide basic explanations', description: 'Offer a brief explanation for each AI decision: Why was the decision made? What factors were decisive? Use simple feature importance visualizations.' },
          ],
          quickWins: [
            'Create a user-friendly system description (1 page)',
            'Display an "AI-powered" badge at all relevant points',
            'Show top 3 influencing factors for each decision',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Implement systematic XAI methods', description: 'Choose suitable explainability methods: LIME for local explanations, SHAP for feature importances, counterfactual explanations for alternative scenarios. Implement at least two complementary methods.' },
            { title: 'Fulfill information obligations under Art. 13(3)', description: 'Provide: provider contact details, system purpose, accuracy metrics, known risks, required human oversight, expected lifetime.' },
            { title: 'Document capabilities and limitations', description: 'Create a detailed overview: scope of application, performance limits, scenarios with limited reliability, known weaknesses, conditions for optimal functionality.' },
            { title: 'Document governance processes transparently', description: 'Make your governance processes transparent: Who decides what? How are complaints handled? What control mechanisms exist?' },
          ],
          quickWins: [
            'Implement SHAP plots as standard explanations',
            'Publish a model card with performance limits',
            'Create a transparency page on website/intranet',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Audience-specific explanations', description: 'Differentiate explanations by target audience: technically detailed for developers, comprehensible summaries for end users, regulatory reports for supervisory authorities.' },
            { title: 'Measure explanation quality', description: 'Measure the quality of your explanations: user comprehension (tests), satisfaction (surveys), correctness (validation against ground truth), consistency.' },
            { title: 'Proactive, context-specific information', description: 'Proactively inform users when system characteristics change, confidence is low, or the system operates outside its optimal range.' },
          ],
          quickWins: [
            'Measure explanation comprehension in user research tests',
            'Display confidence indicator for each system output',
            'Provide changelog for users on system updates',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Adaptive explainability', description: 'Automatically adapt explanations to the knowledge level and needs of each user. Use feedback loops to continuously improve explanations.' },
            { title: 'Interactive information systems', description: 'Enable users to explore interactively: what-if analyses, comparison scenarios, detailed breakdowns. Implement conversational XAI.' },
          ],
          quickWins: [
            'Systematically evaluate user feedback on explanations',
            'Prototype an interactive explorer for model decisions',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'Establish transparency benchmarks', description: 'Compare your transparency measures with industry standards. Publish an annual transparency report.' },
            { title: 'Anticipate regulatory developments', description: 'Track the development of harmonized standards for Art. 13 and proactively adjust your transparency measures.' },
          ],
          quickWins: [
            'Publish an annual AI transparency report',
            'Establish a user advisory board for transparency feedback',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 13 Transparency (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'SHAP \u2014 SHapley Additive exPlanations', url: 'https://shap.readthedocs.io/' },
        { title: 'LIME \u2014 Local Interpretable Model-agnostic Explanations', url: 'https://github.com/marcotcr/lime' },
        { title: 'AlgorithmWatch \u2014 Automated Decision-Making Systems', url: 'https://algorithmwatch.org/de/' },
      ],
    },
    d5: {
      name: 'Human Oversight',
      article: 'Art. 14',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Identify human-in-the-loop requirements', description: 'Determine where human oversight is required. Distinguish: HITL (Human-in-the-Loop, human decides), HOTL (Human-on-the-Loop, human monitors), HOCL (Human-out-of-Command, human can intervene).' },
            { title: 'Provide emergency shutdown', description: 'Implement an immediately accessible option to stop or deactivate the AI system. Document the process and ensure it can be used without specialized technical knowledge.' },
            { title: 'Conduct basic training', description: 'Train all oversight personnel on: system functionality, system limitations, error recognition indicators, escalation paths, use of shutdown mechanisms.' },
          ],
          quickWins: [
            'HITL/HOTL/HOCL assignment for all AI decisions',
            'Provide kill switch and documentation',
            'Develop basic training for oversight personnel (2h)',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Systematize oversight concept', description: 'Document a formal oversight concept: Who monitors when? What information is available? Which decisions may be automated? How is escalation handled?' },
            { title: 'Define competency profiles', description: 'Create competency profiles for oversight personnel: technical understanding, domain knowledge, awareness of system limitations, ethical competency. Define training programs.' },
            { title: 'Escalation and intervention processes', description: 'Define clear processes: At which signals is escalation triggered? To whom? In what timeframe? How are corrections documented? How is effectiveness validated?' },
            { title: 'Automation bias prevention', description: 'Implement systematic measures: decision support instead of decision-making, regular calibration exercises, diversity in oversight teams, mandatory independent review before confirmation.' },
          ],
          quickWins: [
            'Formalize oversight concept as a document',
            'Monthly calibration exercise for oversight personnel',
            'Automation bias awareness training (semi-annually)',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Measure override rate and quality', description: 'Systematically capture and analyze: How often do oversight personnel intervene? Was the intervention correct? How long is the reaction time? Do oversight personnel reliably detect errors?' },
            { title: 'Automated intervention mechanisms', description: 'Implement automatic threshold-based interventions: system pauses at low confidence, escalates on unusual inputs, alerts on drift.' },
            { title: 'Measure bias prevention effectiveness', description: 'Measure whether your automation bias countermeasures are working: compare override rates before and after training, measure correctness of override decisions.' },
          ],
          quickWins: [
            'Set up an override dashboard with trend analysis',
            'Automatic pause when confidence < threshold',
            'Quarterly analysis of override quality',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Adaptive oversight mechanisms', description: 'Dynamically adjust the level of human oversight: more oversight for unknown scenarios, less for well-understood ones. Use conformal prediction for calibrated uncertainty estimates.' },
            { title: 'Predictive escalation', description: 'Implement systems that predict potential problems and proactively escalate before they occur. Use patterns from historical override data.' },
          ],
          quickWins: [
            'Dynamic HITL thresholds based on confidence',
            'Learning escalation from historical data',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'Research human-machine teaming', description: 'Evaluate new approaches for optimal task allocation between humans and AI. Leverage research findings from human-AI interaction research.' },
            { title: 'Cognitive load management', description: 'Optimize the cognitive load of oversight personnel: information presentation, alert fatigue prevention, task rotation.' },
          ],
          quickWins: [
            'Conduct alert fatigue analysis and prioritize alerts',
            'Conduct ergonomics review of oversight tools',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 14 Human Oversight (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'ISO/IEC TR 24028 \u2014 AI Trustworthiness', url: 'https://www.iso.org/standard/77608.html' },
        { title: 'Stanford HAI \u2014 Human-Centered AI', url: 'https://hai.stanford.edu/' },
        { title: 'EU HLEG \u2014 Ethics Guidelines for Trustworthy AI', url: 'https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai' },
      ],
    },
    d6: {
      name: 'Technical Robustness',
      article: 'Art. 15',
      levels: {
        '1to2': {
          target: 'From Initial to Managed',
          steps: [
            { title: 'Define baseline metrics', description: 'Define basic performance metrics: Accuracy, Precision, Recall, F1 Score (classification) or MAE, RMSE (regression). Measure these on a separate test dataset and document the results.' },
            { title: 'Basic error handling', description: 'Implement robust error handling: graceful degradation on missing inputs, meaningful error messages, fallback to rule-based systems on system failure.' },
            { title: 'IT security baseline', description: 'Ensure standard IT security measures are applied: access controls, encryption, regular updates, network segmentation for the AI system.' },
          ],
          quickWins: [
            'Document and version test set performance',
            'Implement fallback message on system failure',
            'Restrict AI system access to authorized users',
          ],
        },
        '2to3': {
          target: 'From Managed to Defined',
          steps: [
            { title: 'Comprehensive metrics with thresholds', description: 'Extend your metrics: fairness metrics per subgroup, calibration, robustness against noise, latency metrics. Define minimum thresholds below which the system must not be deployed.' },
            { title: 'Systematic robustness testing', description: 'Implement: adversarial tests (FGSM, PGD), perturbation tests, stress tests with extreme values, out-of-distribution detection. Integrate these into your CI/CD pipeline.' },
            { title: 'AI-specific security measures', description: 'Implement: input validation against adversarial inputs, model integrity checks (hash validation), protection against model extraction, monitoring for unusual query patterns.' },
            { title: 'Monitoring and drift detection', description: 'Set up automated monitoring: data drift (PSI, KS test), concept drift (performance degradation), feature drift. Define thresholds and alerting.' },
            { title: 'Define fallback mechanisms', description: 'Define graduated fallback processes: warning \u2192 restriction \u2192 rule-based fallback \u2192 manual process \u2192 system shutdown. Test these regularly.' },
          ],
          quickWins: [
            'Deploy Adversarial Robustness Toolbox (ART)',
            'Set up PSI/KS test for data drift as a cron job',
            'Document and annually test fallback process',
          ],
        },
        '3to4': {
          target: 'From Defined to Measured',
          steps: [
            { title: 'Real-time performance monitoring', description: 'Implement dashboards with real-time metrics: inference latency, throughput, error rate, drift indicators, anomaly scores. Set automatic alerts on threshold breaches.' },
            { title: 'Red teaming and penetration tests', description: 'Conduct regular adversarial security tests: model inversion, membership inference, data poisoning simulations. Document findings and remediations.' },
            { title: 'Automated fallback testing', description: 'Test fallback mechanisms automatically: chaos engineering for AI (random fault injection), load testing, failover tests.' },
          ],
          quickWins: [
            'Grafana/Prometheus for AI metrics dashboard',
            'Introduce quarterly red teaming',
            'Chaos engineering tests for fallback validation',
          ],
        },
        '4to5': {
          target: 'From Measured to Optimizing',
          steps: [
            { title: 'Predictive monitoring', description: 'Implement ML-based monitoring that predicts performance degradation before it occurs. Use time-series forecasting on metrics data.' },
            { title: 'Automatic model updating', description: 'Implement secure, automated retraining pipelines: triggers on drift detection, automated validation, canary deployments, automated rollback.' },
            { title: 'Resilient architecture', description: 'Implement self-healing mechanisms: automatic failover, load balancing, circuit breaker pattern, automatic scaling under load.' },
          ],
          quickWins: [
            'Time-series forecasting on performance metrics',
            'Canary deployment for model updates',
            'Circuit breaker pattern for external dependencies',
          ],
        },
        optimizing: {
          target: 'Maintain Level 5: Pre-emptive measures',
          steps: [
            { title: 'Track emerging threats', description: 'Actively track new threats to AI systems: new adversarial attack techniques, supply chain risks for ML models, security vulnerabilities in ML frameworks.' },
            { title: 'Advance zero-trust for AI', description: 'Extend zero-trust principles to AI components: every model prediction is validated, every input is checked, no implicit trust.' },
          ],
          quickWins: [
            'Subscribe to ML security newsletter/feed',
            'Automate dependency scanning for ML packages',
          ],
        },
      },
      sources: [
        { title: 'EU AI Act \u2014 Art. 15 Accuracy, Robustness, Cybersecurity (Full Text)', url: 'https://eur-lex.europa.eu/legal-content/DE/TXT/?uri=CELEX:32024R1689' },
        { title: 'ENISA \u2014 Securing Machine Learning Algorithms', url: 'https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms' },
        { title: 'IBM Adversarial Robustness Toolbox (ART)', url: 'https://github.com/Trusted-AI/adversarial-robustness-toolbox' },
        { title: 'MITRE ATLAS \u2014 Adversarial Threat Landscape for AI', url: 'https://atlas.mitre.org/' },
      ],
    },
  },
  dimensions: {
    D1: {
      name: 'Risk Management',
      description: 'Establishment and maintenance of an AI-specific risk management system across the entire lifecycle — from systematic identification through assessment to continuous validation pursuant to Art. 9.',
      criteria: {
        'D1.1': { name: 'AI-specific risk management system', question: 'Does a documented, AI-specific risk management system exist in your organization?', indicators: { '1': 'No formal risk management system in place', '2': 'Basic risk consideration, but not AI-specific', '3': 'Documented AI risk management system with defined processes', '4': 'Systematically measured and monitored risk management system with KPIs', '5': 'Continuously optimized system with best-practice sharing' } },
        'D1.2': { name: 'Systematic risk identification', question: 'Is systematic risk identification conducted across the entire AI lifecycle?', indicators: { '1': 'Ad-hoc risk identification', '2': 'Risk identification in individual phases', '3': 'Systematic risk identification across all lifecycle phases', '4': 'Quantitative risk analyses with defined metrics', '5': 'Predictive risk identification with continuous adaptation' } },
        'D1.3': { name: 'Risk assessment methodology', question: 'What methodology is used for risk assessment?', indicators: { '1': 'No formal methodology', '2': 'Simple qualitative assessment', '3': 'Documented methodology (likelihood × severity)', '4': 'Quantitative assessment with statistical methods', '5': 'Advanced methods with scenario-based analysis' } },
        'D1.4': { name: 'Risk treatment and residual risk', question: 'Are risk treatment measures and residual risk acceptance criteria defined?', indicators: { '1': 'No defined measures', '2': 'Individual measures without systematic assignment', '3': 'Defined measures with documented residual risk acceptance', '4': 'Systematic measure tracking with effectiveness measurement', '5': 'Optimized risk treatment with automated monitoring' } },
        'D1.5': { name: 'Testing procedures for risk validation', question: 'Are regular testing procedures conducted to validate risk assessments?', indicators: { '1': 'No testing procedures', '2': 'Occasional manual tests', '3': 'Regular, documented testing procedures', '4': 'Automated test suites with defined thresholds', '5': 'Continuous testing pipeline with regression tests' } },
        'D1.6': { name: 'Continuous updating', question: 'Is the risk management system continuously updated?', indicators: { '1': 'No updating', '2': 'Event-driven updating', '3': 'Regular review cycles (e.g., quarterly)', '4': 'Event-driven plus regular updating with metrics', '5': 'Real-time monitoring with automated adaptation' } },
      },
    },
    D2: {
      name: 'Data Governance',
      description: 'Quality assurance of training, validation and test data including bias detection, data lineage tracking and GDPR-compliant data governance pursuant to Art. 10.',
      criteria: {
        'D2.1': { name: 'Data quality standards', question: 'Do defined data quality criteria exist for AI training, validation and test data?', indicators: { '1': 'No data quality standards', '2': 'Basic quality checks', '3': 'Documented standards with defined metrics', '4': 'Automated quality monitoring with dashboards', '5': 'Predictive quality assurance with continuous optimization' } },
        'D2.2': { name: 'Bias detection and correction', question: 'Do systematic procedures exist for detecting and minimizing biases?', indicators: { '1': 'No bias checks', '2': 'Manual review of individual datasets', '3': 'Systematic bias analysis with defined methods', '4': 'Automated bias detection with fairness metrics', '5': 'Continuous bias monitoring with automated correction' } },
        'D2.3': { name: 'Data Lineage', question: 'Is data provenance, processing and usage documented?', indicators: { '1': 'No documentation', '2': 'Partial documentation of data sources', '3': 'Complete data lineage documentation', '4': 'Automated lineage capture with visualization', '5': 'End-to-end lineage with impact analysis' } },
        'D2.4': { name: 'Data protection integration', question: 'Are data protection requirements (GDPR) integrated into data governance?', indicators: { '1': 'No integration', '2': 'Basic GDPR measures', '3': 'Systematic integration with privacy-by-design', '4': 'Automated compliance checks with auditing', '5': 'Proactive data protection governance with impact assessments' } },
        'D2.5': { name: 'Continuous data quality assurance', question: 'Do cycles for continuous data quality assurance exist?', indicators: { '1': 'No continuous assurance', '2': 'Reactive quality corrections', '3': 'Regular quality reviews', '4': 'Proactive quality assurance with monitoring', '5': 'Automated quality optimization with ML-supported detection' } },
      },
    },
    D3: {
      name: 'Documentation',
      description: 'Complete technical documentation pursuant to Annex IV as well as automated event logging, versioning and evidence management pursuant to Art. 11–12.',
      criteria: {
        'D3.1': { name: 'Technical documentation', question: 'Is the technical documentation complete and up to date?', indicators: { '1': 'No or minimal documentation', '2': 'Basic documentation, not current', '3': 'Complete, current documentation pursuant to Annex IV', '4': 'Automatically generated and updated documentation', '5': 'Living documentation with automatic validation' } },
        'D3.2': { name: 'Automated logging', question: 'Are systems for automated recording of system events implemented?', indicators: { '1': 'No logging', '2': 'Basic logging without structure', '3': 'Structured logging of all relevant events', '4': 'Centralized logging with analysis tools', '5': 'Intelligent logging with automated anomaly detection' } },
        'D3.3': { name: 'Versioning', question: 'Is end-to-end versioning and change tracking implemented?', indicators: { '1': 'No versioning', '2': 'Manual versioning of individual components', '3': 'Systematic versioning of all artifacts', '4': 'Automated versioning with dependency management', '5': 'Complete reproducibility of all model versions' } },
        'D3.4': { name: 'Conformity evidence', question: 'Are conformity evidence and regulatory reports systematically created?', indicators: { '1': 'No conformity evidence', '2': 'Ad-hoc created evidence', '3': 'Systematic creation pursuant to regulatory requirements', '4': 'Automated evidence creation with audit trail', '5': 'Proactive compliance reporting with early warning' } },
        'D3.5': { name: 'Documentation management', question: 'How is the accessibility and management of documentation ensured?', indicators: { '1': 'Unstructured storage', '2': 'Central storage without access management', '3': 'Structured DMS with access rights', '4': 'Integrated DMS with workflow support', '5': 'AI-powered knowledge management with automatic classification' } },
      },
    },
    D4: {
      name: 'Transparency',
      description: 'Explainability of AI decisions, information obligations towards users, labeling of AI-generated content and process transparency pursuant to Art. 13.',
      criteria: {
        'D4.1': { name: 'Explainability', question: 'How is the explainability of system functionality and decision logic ensured?', indicators: { '1': 'No explainability measures', '2': 'Basic model description', '3': 'Systematic explainability with defined methods', '4': 'Audience-specific explanations with validation', '5': 'Adaptive explainability with user feedback integration' } },
        'D4.2': { name: 'Information obligations', question: 'Are information obligations towards operators and users fulfilled?', indicators: { '1': 'No information provision', '2': 'Basic product information', '3': 'Complete information pursuant to Art. 13(3)', '4': 'Proactive, audience-appropriate information', '5': 'Interactive information systems with feedback' } },
        'D4.3': { name: 'Communication of limitations', question: 'Are the capabilities, limitations and risks of the AI system transparently communicated?', indicators: { '1': 'No communication', '2': 'General notices', '3': 'Documented capabilities and limitations', '4': 'Context-specific communication with scenarios', '5': 'Dynamic communication with real-time updates' } },
        'D4.4': { name: 'AI labeling', question: 'Are AI-generated content and AI interactions labeled?', indicators: { '1': 'No labeling', '2': 'Partial labeling', '3': 'Systematic labeling of all AI interactions', '4': 'Automated labeling with metadata', '5': 'Comprehensive labeling with provenance tracking' } },
        'D4.5': { name: 'Process transparency', question: 'Is the governance process itself transparently documented?', indicators: { '1': 'No process transparency', '2': 'Basic process description', '3': 'Documented governance processes with roles', '4': 'Transparent processes with audit capability', '5': 'Complete process transparency with public reporting' } },
      },
    },
    D5: {
      name: 'Human Oversight',
      description: 'Design for effective human oversight (HITL/HOTL), qualified oversight personnel, intervention mechanisms and prevention of automation bias pursuant to Art. 14.',
      criteria: {
        'D5.1': { name: 'Design for human oversight', question: 'Is the AI system designed for effective human oversight?', indicators: { '1': 'No consideration of human oversight', '2': 'Basic monitoring capability', '3': 'Systematic human-in/on-the-loop design', '4': 'Context-adaptive oversight mechanisms', '5': 'Optimized human-machine interaction with feedback loops' } },
        'D5.2': { name: 'Qualification of oversight personnel', question: 'Are oversight personnel sufficiently qualified?', indicators: { '1': 'No defined qualifications', '2': 'Basic instruction', '3': 'Defined competency profiles and training programs', '4': 'Certified qualification with regular refresher training', '5': 'Continuous competency development with performance monitoring' } },
        'D5.3': { name: 'Intervention mechanisms', question: 'Do clear mechanisms for intervention, correction and shutdown exist?', indicators: { '1': 'No intervention capability', '2': 'Basic shutdown capability', '3': 'Defined escalation and intervention processes', '4': 'Automated intervention mechanisms with thresholds', '5': 'Adaptive intervention systems with predictive escalation' } },
        'D5.4': { name: 'Prevention of automation bias', question: 'Are measures taken against automation bias and algorithm aversion?', indicators: { '1': 'No measures', '2': 'Awareness training', '3': 'Systematic measures (e.g., decision support instead of decision-making)', '4': 'Measured effectiveness of bias prevention', '5': 'Evidence-based, continuously optimized prevention measures' } },
        'D5.5': { name: 'Documentation of oversight decisions', question: 'Are oversight decisions documented and reviewed?', indicators: { '1': 'No documentation', '2': 'Ad-hoc documentation', '3': 'Systematic documentation of all oversight decisions', '4': 'Automated capture with analysis dashboard', '5': 'Learning documentation with pattern recognition' } },
      },
    },
    D6: {
      name: 'Technical Robustness',
      description: 'Defined accuracy metrics, robustness against disturbances and adversarial attacks, cybersecurity and continuous monitoring with drift detection pursuant to Art. 15.',
      criteria: {
        'D6.1': { name: 'Accuracy metrics', question: 'Are accuracy metrics and performance thresholds defined?', indicators: { '1': 'No defined metrics', '2': 'Basic accuracy measurement', '3': 'Defined metrics with thresholds for all relevant dimensions', '4': 'Automated performance monitoring with alerting', '5': 'Adaptive thresholds with automatic recalibration' } },
        'D6.2': { name: 'Robustness', question: 'Is the system robust against disturbances, errors and adversarial attacks?', indicators: { '1': 'No robustness measures', '2': 'Basic error handling', '3': 'Systematic robustness tests and measures', '4': 'Automated adversarial tests with metrics', '5': 'Continuous robustness optimization with red teaming' } },
        'D6.3': { name: 'Cybersecurity', question: 'Are cybersecurity measures implemented to protect against manipulation?', indicators: { '1': 'No AI-specific security measures', '2': 'Standard IT security measures', '3': 'AI-specific security measures implemented', '4': 'Comprehensive security framework with penetration tests', '5': 'Zero-trust architecture with continuous threat monitoring' } },
        'D6.4': { name: 'Monitoring and drift detection', question: 'Is continuous monitoring with drift detection implemented?', indicators: { '1': 'No monitoring', '2': 'Manual periodic monitoring', '3': 'Automated monitoring with drift detection', '4': 'Real-time monitoring with automated alerts', '5': 'Predictive monitoring with automatic model updating' } },
        'D6.5': { name: 'Fallback mechanisms', question: 'Do fallback mechanisms and graceful degradation exist?', indicators: { '1': 'No fallback mechanisms', '2': 'Simple error message on system failure', '3': 'Defined fallback processes with graceful degradation', '4': 'Automated fallback systems with monitoring', '5': 'Resilient architecture with self-healing mechanisms' } },
      },
    },
  },
  recommendations: {
    D1: 'Establish a formalized AI risk management system with defined processes for risk identification, assessment and treatment across the entire AI lifecycle.',
    D2: 'Implement systematic data governance processes including data quality standards, bias detection procedures and end-to-end data lineage documentation.',
    D3: 'Complete the technical documentation, establish automated logging systems and implement an end-to-end versioning strategy.',
    D4: 'Strengthen transparency through explainability measures, clear information obligations and systematic labeling of AI-generated content.',
    D5: 'Improve human oversight through qualified oversight personnel, clear intervention mechanisms and measures for prevention of automation bias.',
    D6: 'Increase technical robustness through defined performance thresholds, cybersecurity measures, continuous monitoring and fallback mechanisms.',
  },
  compliance: {
    compliant: 'Compliant',
    partial: 'Partially compliant',
    nonCompliant: 'Non-compliant',
  },
  executive: {
    label: 'Executive Summary',
    systemReaches: 'Your AI system \u201c{system}\u201d achieves an overall maturity of {score}/5.0 (Level: {label}).',
    defaultSystem: 'AI System',
    priorityAction: 'Priority action areas',
    and: 'and',
    riskNote: 'Assessed as: {category}',
  },
  scoring: {
    title: 'Scoring Methodology',
    howCalculated: 'How your score was calculated',
    dimFormula: 'Dimension score = Average of rated criteria (N/A excluded)',
    overallFormula: 'Overall score = Weighted average of 6 dimensions (equal weights)',
    gapThreshold: 'Gap threshold = 3.0 (\u201cDefined\u201d per CMMI) \u2014 the EU AI Act requires defined processes for high-risk systems',
    transparency: 'Transparency is a core principle of the EU AI Act (Art. 13). This methodology is based on the CMMI maturity model, adapted for the requirements of Art. 9\u201315.',
  },
  articleCompliance: {
    title: 'Compliance Status by EU AI Act Articles',
  },
  heatmap: {
    title: 'Compliance Heatmap (Criteria Matrix)',
  },
  maturityLevels: {
    1: 'Initial',
    2: 'Managed',
    3: 'Defined',
    4: 'Measured',
    5: 'Optimizing',
  },
  roadmap: {
    title: 'Compliance Roadmap',
    month: 'Month',
    phase1: 'Phase 1 (Month 1\u20133)',
    phase2: 'Phase 2 (Month 3\u20136)',
    phase3: 'Phase 3 (Month 6\u201312)',
    phase1Desc: 'Address critical gaps',
    phase2Desc: 'Address significant gaps',
    phase3Desc: 'Optimize moderate gaps',
    noGaps: 'No gaps \u2014 all dimensions compliant.',
  },
  effort: {
    label: 'Estimated Effort',
    D1: { hours: '80\u2013120h', weeks: '4\u20138 weeks', size: 'L' },
    D2: { hours: '60\u2013100h', weeks: '3\u20136 weeks', size: 'M\u2013L' },
    D3: { hours: '40\u201380h', weeks: '2\u20134 weeks', size: 'M' },
    D4: { hours: '60\u201390h', weeks: '3\u20136 weeks', size: 'M' },
    D5: { hours: '40\u201370h', weeks: '2\u20134 weeks', size: 'S\u2013M' },
    D6: { hours: '80\u2013120h', weeks: '4\u20138 weeks', size: 'L' },
  },
  criteriaRec: {
    title: 'Criteria-Level Recommendations',
    noCriteria: 'No criteria below threshold.',
    D1_1: 'Create a documented AI risk management system with defined roles, processes and review cycles.',
    D1_2: 'Implement systematic risk identification across all lifecycle phases (development, deployment, operation).',
    D1_3: 'Define a formal risk assessment methodology (probability × severity) based on ISO 31000.',
    D1_4: 'Establish risk treatment measures and residual risk acceptance criteria and document them.',
    D1_5: 'Establish regular, documented test procedures to validate your risk assessments.',
    D1_6: 'Introduce regular review cycles (min. quarterly) for your risk management system.',
    D2_1: 'Define measurable data quality criteria (completeness, timeliness, consistency) for your AI datasets.',
    D2_2: 'Implement systematic bias analyses with defined fairness metrics for your training data.',
    D2_3: 'Document complete data lineage from source to model usage.',
    D2_4: 'Systematically integrate GDPR requirements into your data governance processes (Privacy by Design).',
    D2_5: 'Establish regular data quality reviews and define correction processes.',
    D3_1: 'Create complete technical documentation in accordance with Annex IV EU AI Act.',
    D3_2: 'Implement structured logging of all relevant system events and decisions.',
    D3_3: 'Introduce systematic versioning of all model artifacts, data and configurations.',
    D3_4: 'Create systematic conformity evidence according to regulatory requirements.',
    D3_5: 'Set up a structured document management system with access controls.',
    D4_1: 'Implement explainability measures (e.g. SHAP, LIME) for your AI decisions.',
    D4_2: 'Fulfill information obligations under Art. 13(3) towards operators and users.',
    D4_3: 'Document capabilities, limitations and known risks of your AI system transparently.',
    D4_4: 'Systematically label all AI-generated content and AI interactions.',
    D4_5: 'Document your governance processes transparently with defined roles and responsibilities.',
    D5_1: 'Design your AI system with Human-in-the-Loop (HITL) or Human-on-the-Loop (HOTL) mechanisms.',
    D5_2: 'Define competence profiles and establish training programs for oversight personnel.',
    D5_3: 'Define clear escalation and intervention processes including shutdown mechanisms.',
    D5_4: 'Implement measures against automation bias (e.g. decision support instead of decision making).',
    D5_5: 'Systematically document all oversight decisions and their rationale.',
    D6_1: 'Define accuracy metrics and performance thresholds for all relevant dimensions of your system.',
    D6_2: 'Conduct systematic robustness testing (stress tests, edge cases, adversarial scenarios).',
    D6_3: 'Implement AI-specific cybersecurity measures beyond standard IT security.',
    D6_4: 'Establish automated monitoring with drift detection for your AI system.',
    D6_5: 'Define fallback processes and graceful degradation mechanisms for system failures.',
  },
  successCriteria: {
    title: 'Success Criteria',
    description: 'Verifiable criteria for the target level:',
  },
  mapping: {
    title: 'Dimension-Article Mapping',
    subtitle: 'Mapping of governance dimensions to EU AI Act articles',
    dimension: 'Dimension',
    primary: 'Primary mapping',
    secondary: 'Secondary relation',
    none: 'No relation',
    art9: 'Risk Management',
    art10: 'Data Governance',
    art11: 'Documentation',
    art13: 'Transparency',
    art14: 'Human Oversight',
    art15: 'Robustness',
  },
  timeline: {
    title: 'EU AI Act Compliance Timeline',
    subtitle: 'Key deadlines for EU AI Act implementation',
    m1: 'Prohibited AI practices (Art. 5) come into force. Inadmissible systems must be shut down.',
    m2: 'Governance obligations (Art. 17) and codes of conduct apply. Notified bodies are accredited.',
    m3: 'Full high-risk requirements (Art. 9\u201315) apply. Conformity assessment mandatory.',
    m4: 'Full application of all provisions. All AI systems must be compliant.',
    yourDeadline: 'Your deadline',
    remaining: 'Remaining time',
    remainingText: 'approx. {months} months ({days} days) until relevant deadline',
  },
  glossary: {
    title: 'Glossary',
    subtitle: 'Key terms from EU AI Act and AI governance',
    searchPlaceholder: 'Search term...',
    noResults: 'No results found.',
    terms: 'terms',
    cmmi: { term: 'CMMI', def: 'Capability Maturity Model Integration \u2014 a maturity model for assessing and improving processes. Levels 1 (Initial) to 5 (Optimizing).' },
    highRisk: { term: 'High-Risk AI System', def: 'AI system per Annex III EU AI Act, deployed in sensitive areas (e.g. credit scoring, recruitment). Subject to full requirements of Art. 9\u201315.' },
    limitedRisk: { term: 'Limited Risk AI System', def: 'AI system per Art. 50, subject to transparency obligations (e.g. chatbots, emotion recognition, deepfakes).' },
    minimalRisk: { term: 'Minimal Risk', def: 'AI systems without specific obligations under the EU AI Act (e.g. spam filters, recommendation systems).' },
    conformityAssessment: { term: 'Conformity Assessment', def: 'Procedure to verify whether a high-risk AI system meets EU AI Act requirements (Art. 43).' },
    notifiedBody: { term: 'Notified Body', def: 'Organization designated by member states to conduct conformity assessments for certain high-risk AI systems.' },
    postMarket: { term: 'Post-Market Monitoring', def: 'Monitoring of an AI system after placing on the market to ensure ongoing conformity (Art. 72).' },
    hitl: { term: 'HITL (Human-in-the-Loop)', def: 'Human is directly involved in the decision process and must confirm or reject each AI decision.' },
    hotl: { term: 'HOTL (Human-on-the-Loop)', def: 'Human oversees the AI system and can intervene when needed, but the system makes decisions autonomously.' },
    automationBias: { term: 'Automation Bias', def: 'Tendency of humans to over-rely on automated systems and neglect their own judgment.' },
    dataGovernance: { term: 'Data Governance', def: 'Systematic management of data quality, availability, integrity and security across the data lifecycle.' },
    biasDetection: { term: 'Bias Detection', def: 'Systematic procedures for identifying biases in training data or model outputs.' },
    dataLineage: { term: 'Data Lineage', def: 'Tracking the origin, processing and usage of data from source to end product.' },
    explainability: { term: 'Explainability', def: 'Ability to make AI decisions comprehensible to humans. Methods: SHAP, LIME, Attention Maps.' },
    gracefulDegradation: { term: 'Graceful Degradation', def: 'Ability of a system to maintain limited functionality during partial failures.' },
    driftDetection: { term: 'Drift Detection', def: 'Monitoring procedures to detect changes in data distributions or model behavior over time.' },
    adversarial: { term: 'Adversarial Attacks', def: 'Targeted manipulation of AI systems using specially crafted input data to produce erroneous results.' },
    riskAppetite: { term: 'Risk Appetite', def: 'The level of risk an organization is willing to accept to achieve its objectives.' },
    maturityModel: { term: 'Maturity Model', def: 'Framework for assessing the development status of processes and capabilities on a defined scale.' },
    dimScore: { term: 'Dimension Score', def: 'Average score of all rated criteria of a governance dimension (N/A criteria are excluded).' },
  },
  maturityDist: {
    title: 'Maturity Level Distribution',
    subtitle: 'Distribution of criteria across maturity levels',
    level1: 'Initial',
    level2: 'Managed',
    level3: 'Defined',
    level4: 'Measured',
    level5: 'Optimizing',
    total: '{n} criteria rated',
  },
  consistency: {
    warning: 'Consistency note: Your ratings in {dim} vary significantly ({min}\u2013{max}). Would you like to review them?',
  },
  raci: {
    title: 'Responsibility Matrix (RACI)',
    subtitle: 'Assignment of roles and responsibilities per governance dimension',
    dimension: 'Dimension',
    responsible: 'Responsible',
    accountable: 'Accountable',
    consulted: 'Consulted',
    informed: 'Informed',
  },
  scoping: {
    governanceOfficerLabel: 'Dedicated AI Governance Officer',
    governanceOfficerYes: 'Yes',
    governanceOfficerNo: 'No',
    existingFrameworksLabel: 'Existing Compliance Frameworks',
    existingFrameworksPlaceholder: 'e.g. ISO 27001, SOC2, GDPR',
    numAiSystemsLabel: 'Number of AI Systems in Production',
    numAiSystems1: '1',
    numAiSystems2_5: '2\u20135',
    numAiSystems6_20: '6\u201320',
    numAiSystems20plus: '20+',
  },
  practiceExamples: {
    title: 'Practice Example',
    level2: 'Level 2',
    level3: 'Level 3',
  },
  pdf: {
    reportTitle: 'AI Governance Assessment Report',
    executiveSummary: 'Executive Summary',
    dimensionResults: 'Dimension Results',
    gapAnalysis: 'Gap Analysis & Recommendations',
    methodology: 'Methodology',
    compliant: 'Compliant',
    partial: 'Partial',
    nonCompliant: 'Non-compliant',
    rated: 'rated',
    severity: 'Severity',
    recommendation: 'Recommendation',
    dimension: 'Dimension',
    score: 'Score',
    status: 'Status',
    details: 'Details',
    summaryText: 'The AI system <strong>{name}</strong> achieves an overall maturity of <strong style="color:#5C6BC0;">{score}/5.0</strong> (Level: <strong>{label}</strong>). Of 6 governance dimensions, <strong style="color:#22c55e;">{compliant} are compliant</strong>, <strong style="color:#eab308;">{partial} partially compliant</strong> and <strong style="color:#ef4444;">{nonCompliant} non-compliant</strong>.',
    methodologyText: 'This assessment is based on the CMMI-based maturity model for AI governance according to the EU AI Act (Regulation 2024/1689), Art. 9\u201315. 31 criteria across 6 dimensions are rated on a scale from 1 (Initial) to 5 (Optimizing). Dimension score = average of rated criteria (N/A excluded). Overall score = weighted average of 6 dimensions. Gap threshold for high-risk systems: 3.0 (Defined).',
    generatedAt: 'Generated on {date} | AI Governance Assessment Framework | EU AI Act Compliance',
  },
  cmmiLevels: {
    1: 'Level 1 \u2014 Initial: No formal process exists. Decisions are made ad-hoc and reactively. No documented procedures.',
    2: 'Level 2 \u2014 Managed: Basic processes exist but are not standardized. Results depend on individual efforts.',
    3: 'Level 3 \u2014 Defined: Documented, repeatable processes exist and are followed organization-wide. This is the minimum requirement for EU AI Act conformity.',
    4: 'Level 4 \u2014 Measured: Processes are quantitatively monitored and controlled. KPIs and metrics are defined and actively used.',
    5: 'Level 5 \u2014 Optimizing: Continuous improvement through data-driven optimization. Best-practice sharing and preventive measures.',
  },
} as const

% Anhang H: Evaluationsprotokolle der strukturierten Artefakt-Evaluation (n=8)

\addchap{Anhang H: Evaluationsprotokolle der Artefakt-Evaluation}
\label{app:nutzerfeedbacks}

Die folgenden Protokolle dokumentieren die individuellen Bewertungen und qualitativen Rückmeldungen der acht Evaluierenden im Rahmen der strukturierten Artefakt-Evaluation (vgl. Evaluationsleitfaden in Anhang~\ref{app:interviewleitfaden}). Jede Sitzung umfasste eine begleitete Prototyp-Demonstration (6--7~Min), die standardisierte Likert-Bewertung (E1--E6) sowie eine offene und rollenspezifische Vertiefungsrunde. Die Protokolle geben die Bewertungen und Kernaussagen in zusammengefasster Form wieder.

\textbf{Hinweis zur Struktur:} Die Protokolle folgen der Teilestruktur des Evaluationsleitfadens (Anhang~\ref{app:interviewleitfaden}). Teil~B (Prototyp-Demonstration) entfällt in der Protokollierung, da die begleitete Live-Demonstration für alle Evaluierenden identisch durchgeführt wurde und keinen individuellen Output erzeugt. Protokolliert werden daher nur die Teile mit individuellen Antworten: Teil~A (demografische Einordnung), Teil~C (standardisierte Bewertung) und Teil~D (offene Rückmeldung und rollenspezifische Vertiefung).


%% ============================================================
%% H.1 EXPERTE A -- KI-Governance / Regulatory Affairs
%% ============================================================
\section*{H.1\quad Experte~A -- KI-Governance, Regulatory Affairs}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Senior Consultant KI-Governance \& Regulatory Affairs, internationale Beratungsgesellschaft
    \item[AI-Act-Vertrautheit:] 5/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 5 & -- \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 5 & -- \\
E4 Verständlichkeit & 4 & -- \\
E5 Regulat. Konformität & 4 & -- \\
E6 Praxistauglichkeit & 3 & Grundsätzlich brauchbar, aber Kunden hätten in der Regel bereits GRC-Strukturen im Einsatz. Ein weiteres Standalone-Tool erzeuge ein Silo. Die Anbindung an bestehende Governance-Workflows fehle. \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Die Dimensionsstruktur sei solide. Q1 und Q2 als Querschnittskategorien seien konzeptionell richtig, im Assessment-Ablauf aber unsichtbar. Für die Beratungspraxis werde organisationale Verankerung als expliziter Assessment-Gegenstand benötigt. Überflüssig sei nichts.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Die direkte Verankerung in den Art.~9--15; NIST~AI~RMF oder ISO~42001 seien jurisdiktionsneutral -- für EU-AI-Act-Compliance müsse man den Mapping-Schritt selbst machen. Außerdem die Trennung deterministisches Scoring vs. KI-Analyse -- in der Governance-Beratung sei Reproduzierbarkeit nicht verhandelbar.

\textbf{Wichtigste Verbesserung:} Entscheidungshilfen für die Stufen~2 und~3 -- \glqq Managed\grqq{} vs. \glqq Defined\grqq{} werde in Workshops oft schwammig. Branchenspezifische Beispiele würden helfen. Außerdem eine Konfidenzanzeige bei den KI-generierten Empfehlungen.

\textbf{Empfehlung:} Ja, als Einstiegsinstrument für eine Gap-Analyse; würde es in der Beratung als erstes Assessment einsetzen. Perspektivisch werde die Integration der ausstehenden CEN/CENELEC-Standards wichtig.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Inwieweit bildet das Framework die regulatorische Realität ab -- und wo liegt die größte Diskrepanz zwischen Framework-Anforderung und organisationaler Umsetzbarkeit?}

Die Anforderungsseite stimme. Die Diskrepanz liege auf der Umsetzungsseite: (1)~Unternehmen mit bestehender ISO-27001-Compliance starteten nicht bei Null -- ein Mapping wäre wünschenswert. (2)~Die Gap-Analyse liefere Ampelfarben, aber nicht den Compliance-Hebel. (3)~31~Kriterien für ein Unternehmen mit 80~Mitarbeitenden ohne Compliance-Funktion seien zu viel.


%% ============================================================
%% H.2 EXPERTE B -- Compliance-Management / GRC
%% ============================================================
\section*{H.2\quad Experte~B -- Compliance-Management, GRC}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Teamleiterin Compliance \& Governance, Beratungsunternehmen (Big~Four), CCEP-I-zertifiziert
    \item[AI-Act-Vertrautheit:] 4/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 4 & -- \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 5 & -- \\
E4 Verständlichkeit & 3 & Für Compliance-Fachleute kein Problem. Aber die Fachbereiche würden im Assessment benötigt, und dort werde die Terminologie zur Hürde. Bei D6 -- \glqq Drift-Erkennung\grqq{}, \glqq Redundanzmechanismen\grqq{} -- könne eine Compliance-Teamleiterin nicht allein bewerten. Entscheidungsbäume wären besser als abstrakte Stufenbeschreibungen. \\
E5 Regulat. Konformität & 5 & -- \\
E6 Praxistauglichkeit & 4 & -- \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Die Verknüpfung zur Konformitätsbewertung nach Art.~43 fehle. Es gebe zwei Pfade (Annex~VI und~VII) -- das Framework müsse klarer machen, welche Ergebnisse in welchen Konformitätspfad einfließen.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Die Artikel-Compliance-Visualisierung (\glqq Bei Art.~13 stehen Sie auf Stufe~2\grqq{}). Und die deterministische Bewertungslogik -- Assessment-Ergebnisse müssten nachvollziehbar sein.

\textbf{Wichtigste Verbesserung:} Branchenspezifische Gewichtungen und eine Evidenz-Zuordnung pro Stufe. Stufe~3 bei D3.2 müsse spezifizieren: \glqq Richtlinie vorhanden, mindestens ein System dokumentiert.\grqq{}

\textbf{Empfehlung:} Ja, als Pre-Audit-Instrument. Das könnte ein Einstiegsprodukt für KI-Governance-Beratung werden -- vorausgesetzt, man ergänze branchenspezifische Evidenztypen. Und die harmonisierten CEN/CENELEC-Standards müssten integriert werden.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Wie würden Sie die Ergebnisse in einen bestehenden Compliance-Nachweis integrieren?}

Drei Pfade: (1)~Als Bestandsaufnahme für das CMS nach ISO~37301 -- die PDF-Export-Funktion liefere direkt einen Bericht. (2)~Als Input für den Remediation Plan mit konkreten Maßnahmen, Verantwortlichen und Fristen. (3)~Als wiederkehrendes Monitoring mit quartalsweiser Assessment-Wiederholung. Was fehle: die Unterscheidung Design Effectiveness vs. Operating Effectiveness.


%% ============================================================
%% H.3 EXPERTE C -- KI-Forschung / Responsible AI
%% ============================================================
\section*{H.3\quad Experte~C -- KI-Forschung, Responsible AI}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Postdoktorandin, Forschungsgruppe Responsible AI, technische Universität, publiziert bei ACM~FAccT
    \item[AI-Act-Vertrautheit:] 3/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 4 & -- \\
E2 Vollständigkeit & 3 & Regulatorisch vollständig, aber die ethische Perspektive greife zu kurz. Jobin et al. hätten fünf konvergente Prinzipien identifiziert -- Fairness im Sinne distributiver Gerechtigkeit gehe über Art.~10 hinaus. Ökologische Nachhaltigkeit, Stakeholder-Inklusion fehlten. \\
E3 Konsistenz & 4 & -- \\
E4 Verständlichkeit & 4 & -- \\
E5 Regulat. Konformität & 4 & -- \\
E6 Praxistauglichkeit & 3 & Für größere Organisationen handhabbar. Für NGOs, öffentliche Verwaltung, Forschungsinstitute sei der Aufwand hoch: 31 $\times$ 5 = 155~Indikatoren. \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Contestability, Participatory Governance, ökologische Nachhaltigkeit -- Themen, die im AI~Act kaum vorkämen. Die Fokussierung auf Art.~9--15 ergebe methodisch Sinn, aber eine optionale D7-Erweiterung wäre wünschenswert.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Die methodische Herleitung mittels Inhaltsanalyse nach Mayring sei sauberer als üblich. Die CMMI-Adaption mit transparenter Ausweisung der Interpolationsstufen~2/4 sei methodisch ehrlich.

\textbf{Wichtigste Verbesserung:} Eine modulare Ethik-Erweiterung und eine Brücke zu bestehenden Instrumenten wie dem ALTAI. Im KI-Assistenten seien außerdem explizitere Quellenangaben wünschenswert.

\textbf{Empfehlung:} Mit Einschränkungen. Für AI-Act-Compliance vermutlich das präziseste Instrument; für umfassende Responsible-AI-Strategien fehle die ethische Tiefe.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Sehen Sie konzeptionelle Lücken in der Operationalisierung -- insbesondere bei der Übersetzung ethischer Prinzipien in messbare Indikatoren?}

Bei D1 und D6 funktioniere die Operationalisierung gut. Bei D4 und D5 werde es schwieriger. D5.4 \glqq Prävention von Automation Bias\grqq{} -- in der Human-AI-Interaction-Forschung ein großes Feld; Stufe~3 \glqq systematische Schulungsmaßnahmen\grqq{} erfasse nicht die Komplexität von Trust-Kalibrierung. Grundsätzlicher: Ein Reifegradmodell impliziere linearen Fortschritt. Bei ethischen Aspekten sei das problematisch -- verschiedene Fairness-Metriken könnten mathematisch unvereinbar sein (Chouldechova).


%% ============================================================
%% H.4 EXPERTE D -- Data Science / ML Engineering
%% ============================================================
\section*{H.4\quad Experte~D -- Data Science, ML-Engineering}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Lead ML Engineer, Technologieunternehmen, verantwortlich für MLOps und Model Governance
    \item[AI-Act-Vertrautheit:] 3/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 3 & Für Governance-Fachleute sicher nützlich. Für das ML-Team weniger -- man arbeite mit konkreten Metriken, nicht mit Reifegraden. \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 5 & -- \\
E4 Verständlichkeit & 2 & Eine Sprachwelt, die mit dem Arbeitsalltag wenig zu tun habe -- man denke in Pipelines und Metriken, nicht in Governance-Dimensionen. \\
E5 Regulat. Konformität & 4 & -- \\
E6 Praxistauglichkeit & 2 & 31~Kriterien in juristischer Sprache -- das ML-Team steige nach dem dritten Indikator aus. \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Was fehle: die Brücke zu MLOps. Ein Mapping wäre hilfreich -- D6.3 Stufe~3 bedeute in der Praxis Performance-Monitoring, Stufe~4 automatisiertes Alerting in der Pipeline.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Dass der Score deterministisch sei und nicht von einem LLM abhänge. Und die Konsistenzprüfung, die zeige, wenn Scores sich widersprächen.

\textbf{Wichtigste Verbesserung:} Technische Mapping-Tabellen für D2 und D6 -- nicht \glqq systematische Bias-Erkennung\grqq{}, sondern konkrete Tools und Metriken.

\textbf{Empfehlung:} Für Governance: ja. Für das ML-Team direkt: nein. Aber als Rahmen, der von Compliance durchgeführt und in technische Requirements übersetzt werde -- das gehe.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Sind die technischen Anforderungen (D6, D2) aus ML-Perspektive realistisch bewertbar?}

D6.1 bis D6.3 könne man bewerten. Aber D6.4 Cybersecurity liege beim Security-Team -- die Dimension vermische Verantwortlichkeiten. Bei D2 ähnlich: Data Lineage sei einschätzbar, \glqq Governance personenbezogener Daten\grqq{} müsse der DPO machen. Was störe: fehlende Messkriterien -- \glqq systematische Drift-Erkennung\grqq{} könne alles sein.


%% ============================================================
%% H.5 EXPERTE E -- Management / Strategie
%% ============================================================
\section*{H.5\quad Experte~E -- Management, Strategie}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Bereichsleiter Digitalisierung \& Innovation, erweiterte Geschäftsleitung, mittelständischer Konzern ($\sim$2.500~MA)
    \item[AI-Act-Vertrautheit:] 2/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 4 & -- \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 3 & Die Stufenlogik leuchte ein -- CMMI kenne er aus dem Automotive-Bereich. Ob die Kriterien untereinander widerspruchsfrei seien, könne er als Nicht-Regulierungsexperte nicht abschließend beurteilen. \\
E4 Verständlichkeit & 4 & -- \\
E5 Regulat. Konformität & 4 & -- \\
E6 Praxistauglichkeit & 3 & 90~Minuten Erstassessment seien zu lang. Gebraucht werde eine Executive-Version: 15~Minuten, 10~Kernfragen, One-Pager fürs Board. Und \glqq Stufe~2,3 bei D4\grqq{} müsse übersetzt werden in \glqq mittleres Compliance-Risiko bei Transparenz\grqq{}. \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Was als Manager fehle: eine Dimension, die den Business Case mache. Was koste Non-Compliance? Bußgelder bis 35~Mio.~EUR seien Board-relevant. Governance als \glqq Licence to Operate\grqq{} müsse das Framework kommunizieren.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Radar-Chart und Executive Dashboard seien in einer Vorstandssitzung nutzbar. Gesamtscore mit Ampellogik verstehe jeder.

\textbf{Wichtigste Verbesserung:} Kostenabschätzung pro Gap -- ohne das keine Budgetentscheidung möglich. Und Benchmark-Daten: Wo stünden andere Unternehmen der Branche?

\textbf{Empfehlung:} Ja, aber mit Begleitung. Als Self-Service für Manager nein -- als Instrument, das Compliance durchführe und aufbereite: absolut.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Liefert das Assessment-Ergebnis die Informationen für eine Priorisierung von Governance-Investitionen?}

Die Gap-Analyse sei ein Anfang. Weniger interessiere der exakte Reifegrad als die Frage: Was koste es, von Stufe~2 auf~3 zu kommen -- in Personentagen und Euro? Damit könne man zur Budgetplanung gehen.


%% ============================================================
%% H.6 EXPERTE F -- Datenschutz / DPO
%% ============================================================
\section*{H.6\quad Experte~F -- Datenschutz, DPO}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Externe Datenschutzbeauftragte (TÜV-zertifiziert), Schwerpunkt DSGVO \& KI, betreut mehrere Unternehmen
    \item[AI-Act-Vertrautheit:] 5/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 4 & -- \\
E2 Vollständigkeit & 5 & -- \\
E3 Konsistenz & 5 & -- \\
E4 Verständlichkeit & 5 & -- \\
E5 Regulat. Konformität & 5 & -- \\
E6 Praxistauglichkeit & 4 & -- \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Die DSFA nach Art.~35 DSGVO könnte in D2 stärker hervorgehoben werden -- bei KI-Systemen sei sie fast immer erforderlich, und mit Art.~27 AI~Act komme die Grundrechte-Folgenabschätzung dazu. Aber das sei Feintuning.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Die deterministische Bewertungslogik -- gegenüber einer Aufsichtsbehörde brauche man Reproduzierbarkeit. Die regulatorische Rückverfolgbarkeit spare enorm Zeit. Die Formulierungen seien vergleichbar mit DSGVO-Folgenabschätzungen -- sofort zugänglich.

\textbf{Wichtigste Verbesserung:} DSGVO-Verknüpfungsschicht. Art.~10 Abs.~5 AI~Act erlaube besondere Datenkategorien für Bias-Detection unter DSGVO-Bedingungen. Ein Overlay: \glqq Welche DSGVO-Pflichten korrespondieren mit welchen Dimensionen?\grqq{}

\textbf{Empfehlung:} Ja, definitiv. Gerade für Organisationen mit bestehender DSGVO-Compliance -- der Übergang zum AI~Act sei die häufigste Situation bei ihren Mandanten.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Wie bewerten Sie die Abgrenzung zwischen Framework-Dimensionen und DSGVO-Anforderungen?}

Im Großen und Ganzen gelungen. D2 überlappe mit DSGVO Art.~5, konkretisiere aber KI-spezifisch. D4 Transparenz gehe mit AI-Act-Erklärbarkeit über DSGVO Art.~13/14 hinaus. D5 Menschliche Aufsicht habe in der DSGVO kein echtes Äquivalent -- Art.~22 sei enger gefasst. Wünschenswert: eine Mapping-Tabelle für DPOs und das Spannungsfeld Datenminimierung vs. Repräsentativität nach Art.~10 AI~Act explizit adressieren.


%% ============================================================
%% H.7 EXPERTE G -- KI-Governance / Beratung (KMU-Fokus)
%% ============================================================
\section*{H.7\quad Experte~G -- KI-Governance, Beratung (KMU-Fokus)}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Geschäftsführer, Boutique-Beratung KI-Governance, Fokus KMU und Mittelstand
    \item[AI-Act-Vertrautheit:] 5/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 5 & -- \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 4 & -- \\
E4 Verständlichkeit & 3 & Für Governance-Profis klar. Für die Geschäftsführerin eines Mittelständlers mit 80~Mitarbeitenden -- nein. \glqq Technische Dokumentation nach Art.~11\grqq{} -- was heiße das, wenn man eine SaaS-Lösung einsetze und nichts selbst entwickle? Die Deployer-Perspektive fehle bei einigen Indikatoren. \\
E5 Regulat. Konformität & 5 & -- \\
E6 Praxistauglichkeit & 5 & -- \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Unternehmen wüssten oft nicht, ob sie betroffen seien. Eine vorgelagerte Betroffenheitsanalyse fehle (\glqq Anbieter oder Deployer? Hochrisiko oder nicht?\grqq{}). Art.~4 AI~Literacy könnte als vorgelagertes Kriterium integriert werden.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Sofort einsetzbar; die Reifegradlogik sei aus SPICE oder dem Automotive-Bereich bekannt. Bisher werde mit Excel-Checklisten gearbeitet -- das Framework sei ein Quantensprung.

\textbf{Wichtigste Verbesserung:} Quick Assessment mit 15~Kernkriterien für KMU. 31~Kriterien ohne Compliance-Abteilung seien zu viel. Branchenspezifische Beispiele wären hilfreich.

\textbf{Empfehlung:} Absolut. Die KI-Unterstützung sei für KMU besonders wertvoll -- sie kompensiere fehlende interne Expertise.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Ist das Framework mit 31~Kriterien und 5~Stufen für KMU handhabbar?}

Mit Begleitung: ja. Als Self-Service: nein. Drei Empfehlungen: (1)~Zweistufiger Modus -- Quick Assessment mit 15~Kernkriterien für Deployer, dann bei Bedarf das volle Assessment. (2)~Branchenspezifische Starter-Pakete mit vorkonfigurierten Gewichtungen. (3)~Die KI-Unterstützung gezielt als Wissenssubstitut positionieren -- der kontextuelle Assistent sei für KMU der Enabler.


%% ============================================================
%% H.8 EXPERTE H -- Compliance / Audit
%% ============================================================
\section*{H.8\quad Experte~H -- Compliance, Audit}

\begin{description}[leftmargin=!, labelwidth=4.5cm]
    \item[Rolle:] Senior Auditor, Wirtschaftsprüfungsgesellschaft, Spezialisierung IT-Compliance, CISA-zertifiziert
    \item[AI-Act-Vertrautheit:] 4/5
\end{description}

\subsubsection*{Standardisierte Bewertung (E1--E6)}

\begin{tabular}{lcp{9.5cm}}
\toprule
\textbf{Kriterium} & \textbf{Score} & \textbf{Begründung (bei $\leq$ 3)} \\
\midrule
E1 Nützlichkeit & 4 & -- \\
E2 Vollständigkeit & 4 & -- \\
E3 Konsistenz & 5 & -- \\
E4 Verständlichkeit & 4 & -- \\
E5 Regulat. Konformität & 4 & -- \\
E6 Praxistauglichkeit & 4 & -- \\
\bottomrule
\end{tabular}

\subsubsection*{Fehlende/überflüssige Dimensionen}
Aus Audit-Sicht fehle die Verknüpfung mit Evidenztypen nach ISA~500 (Inspections, Observations, Inquiries, Reperformance). Branchenspezifische Gewichtungen für sektorale Audits wären wünschenswert.

\subsubsection*{Allgemeine Rückmeldung}
\textbf{Größte Stärken:} Reproduzierbarkeit -- wenn zwei Prüfer zu grundlegend verschiedenen Ergebnissen kämen, tauge die Methodik nicht. Die Stufendefinitionen lieferten eine solide Basis. Die 1:1-Zuordnung zu den Artikeln sei das, was man in der Prüfungsvorbereitung brauche -- man könne dem Mandanten zeigen, welcher Artikel welchen Score habe und wo die Lücken lägen.

\textbf{Wichtigste Verbesserung:} Evidenz-Zuordnungstabelle pro Stufe. Stufe~3 bei D3.2 müsse spezifizieren: \glqq Freigegebene Richtlinie, dokumentiertes KI-System, Schulungsnachweis.\grqq{} Branchenspezifische Erwartungshorizonte als konfigurierbare Profile.

\textbf{Empfehlung:} Ja, als Pre-Audit-Instrument. Kein Ersatz für eine formale Prüfung nach Art.~43, aber ein vorbereitender Schritt.

\subsubsection*{Rollenspezifische Vertiefung}
\textit{Eignen sich die Indikatoren als objektiv prüfbare Audit-Kriterien?}

Stufen~1 und~5 seien klar prüfbar -- Negativbefund bzw. dokumentierte KPIs. Bei~2 bis~4 werde es diskussionswürdig -- \glqq Managed\grqq{} vs. \glqq Defined\grqq{} müsse man begründen können. Dafür brauche man Abgrenzungskriterien (vorhanden), definierte Evidenztypen pro Stufe (fehlend), Grenzfall-Beispiele (fehlend). Dass die KI den Score nicht verändern könne, sei als Prüfer entscheidend -- sonst könne man das Ergebnis nicht verwenden. Zudem prüfe das Framework Design Effectiveness -- ob ein Control gelebt werde, lasse sich nur durch operative Prüfungshandlungen feststellen.
